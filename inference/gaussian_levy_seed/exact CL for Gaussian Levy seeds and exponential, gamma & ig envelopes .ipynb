{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "super-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from import_file import import_file\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ambit_stochastics.trawl import trawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-intranet",
   "metadata": {},
   "source": [
    "Let $(l_1,\\ldots,l_n) = {X_s,\\ldots,X_{s\\cdot  n}}$ be the values of a trawl process $X$ observed on a grid with spacing $s$, which is known. We restrict our attention to exponential and gamma envelopes and to Gaussian Levy seeds $L^{'}$ and we try to infer the parameters of the envelope, as well as the mean $\\mu_L $and variace $V_L$ of $L^{'}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-drawing",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Assume the trawl process has one of the following three envelope functions $\\phi \\colon (-\\infty,0] \\to \\mathbb{R}_{\\ge 0}$:\n",
    "\n",
    "$$\n",
    "\\phi_{\\lambda}(t) = \\begin{cases} \n",
    "\\lambda e^{\\lambda t} &\\text{ if } t \\le 0 \\\\ \n",
    "0 &\\text{ if } t > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "for some $\\lambda > 0 $ or \n",
    "\n",
    "$$\n",
    "\\phi_{H,\\delta}(t) = \\begin{cases} \n",
    "\\frac{H}{\\delta} \\left(1-\\frac{s}{\\delta}\\right)^{-(H+1)} &\\text{ if } t \\le 0 \\\\ \n",
    "0 &\\text{ if } t > 0\n",
    "\\end{cases} \n",
    "$$\n",
    "for some $H,\\delta > 0$ or \n",
    "\n",
    "\n",
    "The above functions are normalised such that the Lebesgue measure of the ambit set is $1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-diabetes",
   "metadata": {},
   "source": [
    "Let $\\theta_L = (\\mu_L,V_L), \\theta_{\\text{env}} = \\lambda  $ or $\\theta_{\\text{env}} =(H,\\delta)$ or $\\theta_{\\text{env}}=(\\delta,\\gamma)$. depending on which envelope we use and let $\\theta = (\\theta_L,\\theta_{\\text{env}})$. Consider the composite log-likelihood with lag-indices $0 = j_1 < \\ldots < j_m = k$ given by \n",
    "\\begin{equation}\n",
    "l_J(\\theta)= \\sum_{i=1}^{i=n-k} p(x_i; \\theta)\n",
    "\\end{equation}\n",
    "where $J = (j_1,\\ldots,j_k)$ and $x_i = \\left(l_i=l_{i+j_0},l_{i+j_1},\\ldots,l_{i+j_{m-1}}, l_{i+j_m} =l_{i+k}\\right).$ In this noteobok, we will investigate whether the composite likelihood approach performs better than the method of moments approach and study the effect the choice $k$ has on the inference. Considering more than $k=1$ can be prohibitively expensive for non-Gaussian Levy seeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-shame",
   "metadata": {},
   "source": [
    "\n",
    "#### Composite likelihood \n",
    "\n",
    "Let \n",
    "\\begin{equation}\n",
    "l_k(\\theta) = - \\frac{m(n-k)}{2} \\log{(2 \\pi)}- \\frac{n-k}{2} \\log{(\\det{\\Sigma(\\theta_{\\text{env}})})} - \\frac{1}{2}\\sum_{i=1}^{n-k}\\left(x_i-\\mu\\right)^T\\Sigma^{-1}\\left(x_i-\\mu\\right) \\label{l}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu = [\\mu_L,\\ldots,\\mu_L]$ and $\\Sigma = V_L \\cdot \\tilde{\\Sigma(\\theta_{\\text{env}})}.$ Then\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-interval",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\\label{dl/dmu}\n",
    "\\frac{\\partial l}{\\partial \\mu}  =  \\sum_{i=1}^{n-k}(x_i-\\mu)^T \\Sigma^{-1}  \\text{ hence by chain rule } \\frac{\\partial l}{\\partial \\mu_L} =  \\frac{\\partial l}{\\partial \\mu} \\cdot [\\mu_L,\\ldots,\\mu_L\n",
    "]  =  \\frac{1}{V_L}\\left[Sum\\left(\\left(\\sum_{i=1}^{n-k} x_i^T\\right) \\tilde{\\Sigma}^{-1}\\right) - (n-k) \\mu_L Sum\\left(\\tilde{\\Sigma}^{-1}\\right)\\right] \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-longer",
   "metadata": {},
   "source": [
    "\\begin{equation} \\label{dl\\dV}\n",
    "\\frac{\\partial l}{\\partial V_L} = -\\frac{1}{2} \\frac{\\partial}{\\partial V_L} \\left[\\log{\\left(\\det{\\left(V_L \\cdot \\tilde{\\Sigma}\\right)}\\right)}+  (x-\\mu)^T {(V_L \\cdot \\tilde{\\Sigma})}^{-1}(x-\\mu)\\right] = -\\frac{m(n-k)}{2 V_L} + \\sum_{i=1}^{n-k}\\frac{1}{2V_L^2}(x_i-\\mu)^T {\\tilde{\\Sigma}}^{-1}(x_i-\\mu)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-ceiling",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial l}{\\partial \\theta_{\\text{env}}} = -\\frac{1}{2} \\sum_{i=1}^{n-k} \\left[ \\frac{\\frac{\\partial}{\\partial \\theta_\\text{env}}{\\det\\left(\\tilde{\\Sigma}\\left(\\theta_{\\text{env}}\\right)\\right)}}{det \\left(\\tilde{\\Sigma}\\left(\\theta_{\\text{env}}\\right)\\right)} + \\frac{1}{V_L} \\left(x-\\mu\\right)^T \\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma}^{-1}\\left(\\theta_{\\text{env}}\\right)\\left(x-\\mu\\right) \n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-figure",
   "metadata": {},
   "source": [
    "By [Jacobi's formula](https://en.wikipedia.org/wiki/Jacobi%27s_formula)\n",
    "  $$ \\frac{\\partial}{\\partial \\theta_\\text{env}}{\\det\\left(\\tilde{\\Sigma}\\left(\\theta_{\\text{env}}\\right)\\right)} = \\det\\left(\\tilde{\\Sigma}\\left(\\theta_\\text{env}\\right)\\right)Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\right)$$\n",
    "  hence \n",
    "  $$ \\frac{\\partial}{\\partial \\theta_\\text{env}}{\\log \\det\\left(\\tilde{\\Sigma}\\left(\\theta_{\\text{env}}\\right)\\right)}  = Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\right)\n",
    "  $$ \n",
    "  and further by \n",
    "  $$ 0 = \\frac{\\partial}{\\partial \\theta_\\text{env}} \\left(\\tilde{\\Sigma} \\tilde{\\Sigma}^{-1}\\right) = \\tilde{\\Sigma}\\  \\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma}^{-1} + \\left( \\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma}\\right) \\tilde{\\Sigma}^{-1} $$\n",
    "  we have\n",
    "  $$ \\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma}^{-1} = -\\tilde{\\Sigma}^{-1} \\left(\\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma} \\right) \\ \\tilde{\\Sigma}^{-1} \n",
    "  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-things",
   "metadata": {},
   "source": [
    "hence $$\\frac{\\partial l}{\\partial \\theta_{\\text{env}}} = -\\frac{1}{2}\\left[(n-k) Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\right) - \\frac{1}{V_L} \\sum_{i=1}^{n-k} \\left(x_i-\\mu\\right)^T \\tilde{\\Sigma}^{-1} \\left(\\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma} \\right) \\ \\tilde{\\Sigma}^{-1}\\left(x_i-\\mu\\right) \n",
    "\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-ranch",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "Thus we have the following expressions for the gradients:\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial \\mu} &= \\frac{1}{V_L}\\left[Sum\\left(\\left(\\sum_{i=1}^{n-k} x_i^T\\right) \\tilde{\\Sigma}^{-1}\\right) - (n-k) \\mu_L Sum\\left(\\tilde{\\Sigma}^{-1}\\right)\\right]  \\\\\n",
    "\\frac{\\partial l}{\\partial V_L}  &= -\\frac{m(n-k)}{2 V_L} + \\sum_{i=1}^{n-k}\\frac{1}{2V_L^2}(x_i-\\mu)^T {\\tilde{\\Sigma}}^{-1}(x_i-\\mu)\\\\\n",
    "\\frac{\\partial l}{\\partial \\theta_{\\text{env}}} &= -\\frac{1}{2}\\left[(n-k) Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\right) - \\frac{1}{V_L} \\sum_{i=1}^{n-k} \\left(x_i-\\mu\\right)^T \\tilde{\\Sigma}^{-1} \\left(\\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma} \\right) \\ \\tilde{\\Sigma}^{-1}\\left(x_i-\\mu\\right) \n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-puzzle",
   "metadata": {},
   "source": [
    "We can proceed by BFGS or we can notice that for a given $\\theta_\\text{env},$ the optimal $\\mu(\\theta_\\text{env})$ and $V_L(\\theta_\\text{env})$ are given by\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_L(\\theta_\\text{env}) &=    \\frac{Sum\\left(\\left(\\sum_{i=1}^{n-k} x_i^T\\right) \\tilde{\\Sigma}^{-1}(\\theta_\\text{env})\\right)}{(n-k)Sum\\left(\\tilde{\\Sigma}^{-1}(\\theta_\\text{env})\\right)}  \\\\\n",
    "V_L(\\theta_\\text{env}) &= \\frac{\\sum_{i=1}^{n-k}\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)^T {\\tilde{\\Sigma}}^{-1}(\\theta_\\text{env})\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)}{m(n-k)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-northern",
   "metadata": {},
   "source": [
    "By substituting this back into the likelihood formula we have\n",
    "\\begin{equation}\n",
    "\\tilde{l}_k\\left(\\theta_\\text{env}\\right)  = l_k\\left(\\theta_\\text{env},\\mu_L(\\theta_\\text{env}\\right),V_L(\\theta_\\text{env})) = - \\frac{m(n-k)}{2} \\left(\\log{(2 \\pi)}+1\\right)- \\frac{n-k}{2} \\left[\\log{(\\det{\\tilde{\\Sigma}(\\theta_{\\text{env}})})} + m \\log V_L(\\theta_\\text{env}) \\right] \n",
    "\\end{equation}\n",
    "and thus\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{l}}{\\partial \\theta_{\\text{env}}} = &-\\frac{n-k}{2}   \\left[Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\left(\\theta_{\\text{env}} \\right)\\right) \\right] \\\\\n",
    "&+ \\frac{1}{2 V_L(\\theta_\\text{env})} \\left(\\sum_{i=1}^{n-k}\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)^T \\tilde{\\Sigma}^{-1}\\left(\\theta_{\\text{env}}\\right) \\left(\\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma} \\right) \\ \\tilde{\\Sigma}^{-1}\\left(\\theta_{\\text{env}}\\right)  \\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaptive-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(s,J,envelope,envelope_params):\n",
    "    \"\"\"return overlap area (i.e. correlation) at lags. these are equivalent because the area of the \n",
    "    ambit set is normalised to 1. lags is a SORTED numpy array\"\"\"\n",
    "    \n",
    "    assert envelope in ['gamma','exponential']\n",
    "    assert isinstance(J,np.ndarray)\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        u = envelope_params[0]\n",
    "        areas = np.exp(- u * s * J )\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        H,delta = envelope_params\n",
    "        areas = (1+J*s/delta)**(-H)\n",
    "        \n",
    "    return list(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e0a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.2805658588748474, 0.14242717305466188, 0.08944271909999159]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corr(0.1,np.array((0,1,2)),'exponential',(10,))\n",
    "#import numpy as np\n",
    "corr(1,np.array((0,1,2,3)),'gamma',(1.5,0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "streaming-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_corr(s,J,envelope,envelope_params):\n",
    "    \"\"\"return gradients of the correlation function at lags  with respect to the envelope _params\"\"\"\n",
    "    \n",
    "\n",
    "    assert envelope in ['gamma','exponential']\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        u = envelope_params[0]\n",
    "        gradients_areas = -s * J * np.exp(- u * s * J )\n",
    "        #print(J)\n",
    "        #print(gradients_areas)\n",
    "        return list(gradients_areas)\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        H,delta = envelope_params\n",
    "        \n",
    "        d_areas_d_H     = - np.log(1+J*s/delta) * (1+J*s/delta)**(-H)\n",
    "        d_areas_d_delta =  H * s * J* (1+J*s/delta)**(-H-1)/delta**2     \n",
    "        \n",
    "        return list(d_areas_d_H),list(d_areas_d_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "trying-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigma_tilde_func(s,J,envelope,envelope_params):\n",
    "    \"\"\"inputs a vector = [corr(0),corr(s),...,corr((k-1)*s)] and outputs\n",
    "    Sigma_tilde_ij = overlap area at lag |i-j| = corr(|i-j|*s)\n",
    "    \"\"\"\n",
    "    m = len(J)\n",
    "    overlap_area_vector = corr(s,J,envelope,envelope_params)\n",
    "    Sigma_tilde = [overlap_area_vector[1:i+1][::-1] + overlap_area_vector[:m-i] for i in range(m)]\n",
    "    Sigma_tilde = np.array(Sigma_tilde)\n",
    "    return Sigma_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c52a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70468809],\n",
       "       [0.70468809, 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.linalg.det(Sigma_tilde_func(0.35, np.array((0,1,2,3,5)),'exponential',(1.0,)))\n",
    "Sigma_tilde_func(0.35, np.array((0,1)),'exponential',(1.0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "canadian-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_Sigma_tilde_func(s,J,envelope,envelope_params):\n",
    "    \"\"\"inputs a vector = [corr(0),corr(s),...,corr((k-1)*s)] and outputs the gradients of\n",
    "    Sigma_tilde_ij = overlap area at lag |i-j| = corr(|i-j|*s) with respect to the parameters of the envelope: TO UPDATE\n",
    "    \"\"\"\n",
    "    assert envelope in ['gamma','exponential']\n",
    "    m = len(J)\n",
    "    #k = max(J)\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        d_overlap_area_vector_d_u = gradients_corr(s,J,envelope,envelope_params)\n",
    "        d_Sigma_tilde_d_u = [d_overlap_area_vector_d_u[1:i+1][::-1] + d_overlap_area_vector_d_u[:m-i] for i in range(m)]\n",
    "        d_Sigma_tilde_d_u = np.array(d_Sigma_tilde_d_u)\n",
    "        \n",
    "        return d_Sigma_tilde_d_u\n",
    "    \n",
    "    if envelope == 'gamma':\n",
    "        d_overlap_area_vector_d_H, d_overlap_area_vector_d_delta = gradients_corr(s,J,envelope,envelope_params)\n",
    "        d_Sigma_tilde_d_H = [d_overlap_area_vector_d_H[1:i+1][::-1] + d_overlap_area_vector_d_H[:m-i] for i in range(m)]\n",
    "        d_Sigma_tilde_d_delta = [d_overlap_area_vector_d_delta[1:i+1][::-1] + d_overlap_area_vector_d_delta[:m-i] for i in range(m)]\n",
    "\n",
    "        return np.array(d_Sigma_tilde_d_H),np.array(d_Sigma_tilde_d_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e106d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradients_Sigma_tilde_func(tau,J,envelope,(1.25,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clean-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde):\n",
    "    \"\"\"covariance matrix = Sigma_tilde * V0\n",
    "    Sigma_tilde has to be recomputed if any of the gaussian part params \n",
    "    \n",
    "    mu0 and V0 \n",
    "    \n",
    "    change\"\"\"\n",
    "    m = len(J) \n",
    "    result = np.array([mvn.logpdf(joint, mean = mu_L * np.ones(m) , cov = V_L * Sigma_tilde) #there was a divided by 2 here, i think it was an error\n",
    "     for joint in joints])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-premises",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\mu_L(\\theta_\\text{env}) &=    \\frac{Sum\\left(\\left(\\sum_{i=1}^{n-k} x_i^T\\right) \\tilde{\\Sigma}^{-1}(\\theta_\\text{env})\\right)}{(n-k)Sum\\left(\\tilde{\\Sigma}^{-1}(\\theta_\\text{env})\\right)}  \\\\\n",
    "V_L(\\theta_\\text{env}) &= \\frac{\\sum_{i=1}^{n-k}\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)^T {\\tilde{\\Sigma}}^{-1}(\\theta_\\text{env})\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)}{m(n-k)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{l}_k\\left(\\theta_\\text{env}\\right)  = l_k\\left(\\theta_\\text{env},\\mu_L(\\theta_\\text{env}\\right),V_L(\\theta_\\text{env})) = - \\frac{m(n-k)}{2} \\left(\\log{(2 \\pi)}+1\\right)- \\frac{n-k}{2} \\left[\\log{(\\det{\\tilde{\\Sigma}(\\theta_{\\text{env}})})} + m \\log V_L(\\theta_\\text{env}) \\right] \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greenhouse-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_hat(s,J,envelope,envelope_params,joints,n): \n",
    "    assert max(J) + len(joints) == n\n",
    "    k = max(J)\n",
    "    m = len(J)\n",
    "    Sigma_tilde = Sigma_tilde_func(s,J,envelope,envelope_params)\n",
    "    Sigma_tilde_inverse = np.linalg.pinv(Sigma_tilde)\n",
    "    #print(Sigma_tilde_inverse)\n",
    "    \n",
    "    sum_x_i = np.sum(joints,axis=0)\n",
    "    sum_Sigma_tilde_inverse = np.sum(Sigma_tilde_inverse)\n",
    "    \n",
    "    mu_L = np.sum(sum_x_i @ Sigma_tilde_inverse) / ((n-k)* sum_Sigma_tilde_inverse)\n",
    "    V_L  = np.sum([(joint-mu_L) @ Sigma_tilde_inverse @(joint-mu_L) for joint in joints])/(m*(n-k))\n",
    "    print(mu_L,V_L)\n",
    "    #print(mu_L,V_L)\n",
    "    #only works for \n",
    "   # if envelope == 'exponential':\n",
    "    #    return -k*(n-k)*(np.log(2*np.pi)+1) /2 - (n-k)*((k-1)*np.log(1-np.exp(-envelope_params[0] * s * 2)) + k * np.log(V_L))/2\n",
    "    \n",
    "    #else:\n",
    "    return -m*(n-k)*(np.log(2*np.pi)+1) /2 - (n-k)*(np.log(np.linalg.det(Sigma_tilde)) + m * np.log(V_L))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-northwest",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{l}}{\\partial \\theta_{\\text{env}}} = &-\\frac{n-k}{2}   \\left[Tr\\left(\\tilde{\\Sigma}^{-1}\\frac{\\partial }{\\partial \\theta_{\\text{env}}} \\tilde{\\Sigma}\\left(\\theta_{\\text{env}} \\right)\\right) \\right] \\\\\n",
    "&+ \\frac{1}{2 V_L(\\theta_\\text{env})} \\left(\\sum_{i=1}^{n-k}\\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right)^T \\tilde{\\Sigma}^{-1}\\left(\\theta_{\\text{env}}\\right) \\left(\\frac{\\partial}{\\partial \\theta_\\text{env}} \\tilde{\\Sigma} \\right) \\ \\tilde{\\Sigma}^{-1}\\left(\\theta_{\\text{env}}\\right)  \\left(x_i-[\\mu_L(\\theta_\\text{env}),\\ldots,\\mu_L(\\theta_\\text{env})]\\right) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amateur-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_l_hat(s,J,envelope,envelope_params,joints,n):\n",
    "    Sigma_tilde = Sigma_tilde_func(s,J,envelope,envelope_params)\n",
    "    Sigma_tilde_inverse = np.linalg.pinv(Sigma_tilde)\n",
    "    \n",
    "    gradients_Sigma_tilde = gradients_Sigma_tilde_func(s,J,envelope,envelope_params)\n",
    "    assert max(J) + len(joints) == n\n",
    "    k = max(J)\n",
    "    m = len(J)\n",
    "\n",
    "    sum_x_i = np.sum(joints,axis=0)\n",
    "    sum_Sigma_tilde_inverse = np.sum(Sigma_tilde_inverse)\n",
    "    \n",
    "    mu_L = np.sum(sum_x_i @ Sigma_tilde_inverse) / ((n-k)* sum_Sigma_tilde_inverse)\n",
    "    V_L  = np.sum([(joint-mu_L) @ Sigma_tilde_inverse @(joint-mu_L) for joint in joints])/(m*(n-k))\n",
    "    \n",
    "    #if envelope == 'exponential':\n",
    "    #        \n",
    "    #    #term1 = -(n-k) * np.trace(Sigma_tilde_inverse @ gradients_Sigma_tilde)/2 \n",
    "    #    term1 = -(n-k) * (k-1) * s * np.exp(-2 * envelope_params[0] * s) / (1-np.exp(-2*s*envelope_params[0]))\n",
    "    #    term2 = np.sum([(joint-mu_L) @ Sigma_tilde_inverse @ gradients_Sigma_tilde @  Sigma_tilde_inverse @ (joint-mu_L) for joint in joints]) / (2 * V_L)\n",
    "    #    #print(term1,term3)\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        term1 = -(n-k) * np.trace(Sigma_tilde_inverse @ gradients_Sigma_tilde)/2\n",
    "        term2 = np.sum([(joint-mu_L) @ Sigma_tilde_inverse @ gradients_Sigma_tilde @  Sigma_tilde_inverse @ (joint-mu_L) for joint in joints]) / (2 * V_L)\n",
    "\n",
    "    \n",
    "    elif envelope == \"gamma\":\n",
    "\n",
    "        term1 = np.array([-(n-k) * np.trace(Sigma_tilde_inverse @ i)/2 for i in gradients_Sigma_tilde])\n",
    "        term2 = np.array([np.sum([(joint-mu_L) @ Sigma_tilde_inverse @ i @  Sigma_tilde_inverse @ (joint-mu_L) for joint in joints]) for i in gradients_Sigma_tilde]) / (2 * V_L)\n",
    "                \n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_l_hat(s,J,envelope,x0,joints,n):\n",
    "    #x0 - initial envelope params\n",
    "    #minimize minus the likelihood. \n",
    "    #\n",
    "    fun = lambda x : -l_hat(s,J,envelope,x,joints,n)\n",
    "    jac  = lambda x: -gradients_l_hat(s,J,envelope,x,joints,n)\n",
    "    \n",
    "    if envelope == 'gamma':\n",
    "        bounds  = ((0.001,np.inf),(0.001,np.inf))\n",
    "    elif envelope == 'exponential':\n",
    "        bounds = ((0.001,np.inf),)\n",
    "    \n",
    "    result = minimize(fun = fun,jac= jac, x0=x0, method='L-BFGS-B',bounds=bounds)#, options = {\"maxiter\":200, \"disp\":True})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reported-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_L_V_L(s,J,envelope,envelope_params,joints,n):\n",
    "    Sigma_tilde = Sigma_tilde_func(s,J,envelope,envelope_params)\n",
    "    Sigma_tilde_inverse = np.linalg.pinv(Sigma_tilde)\n",
    "    \n",
    "    #gradients_Sigma_tilde = gradients_Sigma_tilde_func(s,k,envelope,envelope_params)\n",
    "    assert max(J) + len(joints) == n\n",
    "    k = max(J)\n",
    "    m = len(J)\n",
    "    sum_x_i = np.sum(joints,axis=0)\n",
    "    sum_Sigma_tilde_inverse = np.sum(Sigma_tilde_inverse)\n",
    "    \n",
    "    mu_L = np.sum(sum_x_i @ Sigma_tilde_inverse) / ((n-k)* sum_Sigma_tilde_inverse)\n",
    "    V_L  = np.sum([(joint-mu_L) @ Sigma_tilde_inverse @(joint-mu_L) for joint in joints])/(m*(n-k))\n",
    "    return mu_L,V_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bce87e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(l_hat(tau,J,'exponential',[1.5],joints,nr_trawls) - l_hat(tau,J,'exponential',[1.499],joints,nr_trawls)) / 0.001\n",
    "#gradients_l_hat(s,J,envelope,envelope_params,joints,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noble-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that finite differences and theoretical gradients are the same\n",
    "\n",
    "#(l_hat(s,2,'exponential',[1.5],joints) - l_hat(s,2,'exponential',[1.499],joints)) / 0.001\n",
    "#gradients_l_hat(s,2,'exponential',[1.5],joints)\n",
    "\n",
    "\n",
    "#(l_hat(s,2,'gamma',[1.7,2.001],joints) - l_hat(s,2,'gamma',[1.7,2],joints)) / 0.001\n",
    "#(l_hat(s,2,'gamma',[1.7001,2],joints) - l_hat(s,2,'gamma',[1.7,2],joints)) / 0.0001\n",
    "#gradients_l_hat(s,int(2),'gamma',(1.7,2.),joints)\n",
    "#gradients_l_hat(s,k,envelope,(1.2,),joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19416249",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "np.array(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dd380",
   "metadata": {},
   "source": [
    "# TESTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df461daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01069418 0.01144373 0.00109475 0.00130094 0.00641031 0.00548688\n",
      " 0.01137941 0.01128808 0.00677853 0.00377304 0.01052604 0.01328496\n",
      " 0.00837449 0.00825044 0.01323407 0.00733058 0.00758415 0.00660776\n",
      " 0.00869977 0.00975882 0.00986842 0.01278278 0.01346302 0.0131613\n",
      " 0.01049424 0.01110491 0.01239933 0.01114926 0.00384613 0.00385762\n",
      " 0.01123756 0.01059477 0.01091636 0.01118781 0.01010795 0.00602821\n",
      " 0.0072709  0.00569537 0.0054869  0.00887055 0.00806918 0.00344959\n",
      " 0.00526264 0.01014372 0.01170979 0.00243662 0.00292541 0.00044906\n",
      " 0.00036973 0.00442589 0.00472201 0.01020964 0.00632621 0.00448139\n",
      " 0.00696812 0.01283453 0.01011578 0.0099234  0.01226767 0.00527511\n",
      " 0.00433136 0.00556596 0.00406644 0.00957763 0.01048162 0.01174543\n",
      " 0.0129471  0.00223479 0.00235471 0.01177148 0.01191816 0.0132249\n",
      " 0.01254844 0.01271211 0.01269782 0.00954817 0.00938845 0.00545113\n",
      " 0.00527369 0.01349995 0.00705862 0.00244723 0.00707908 0.01123404\n",
      " 0.0115444  0.00467269 0.00480663 0.01242002 0.01224389 0.00888453\n",
      " 0.00699855 0.00870176 0.00092921 0.00080658 0.0052239  0.00969652\n",
      " 0.00736861 0.00724246 0.01120138] \n",
      "\n",
      "-493.72131744446983\n"
     ]
    }
   ],
   "source": [
    "########## test the jax gamma implementation ############\n",
    "values_to_use = np.load('values_to_use.npy')\n",
    "nr_trawls = len(values_to_use)\n",
    "J = np.array((0,1)) \n",
    "joints = [values_to_use[i+J] for i in range(nr_trawls - max(J))]\n",
    "mu_L, V_L = -4.5, 3.5**2\n",
    "envelope = 'gamma'\n",
    "envelope_params = (1.5,0.75)\n",
    "s =1\n",
    "print(np.exp(composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde_func(s,J,envelope,envelope_params))),'\\n')\n",
    "print(np.sum(composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde_func(s,J,envelope,envelope_params))))\n",
    "\n",
    "r = composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde_func(s,J,envelope,envelope_params))\n",
    "\n",
    "\n",
    "delta_x = 10**(-6)\n",
    "r_1 = composite_likelihoods(joints,J,mu_L+delta_x,V_L,Sigma_tilde_func(s,J,envelope,envelope_params))\n",
    "V_L_delta_x = (3.5 + delta_x)**2\n",
    "r_2 = composite_likelihoods(joints,J,mu_L,V_L_delta_x,Sigma_tilde_func(s,J,envelope,envelope_params))\n",
    "r_3 = composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde_func(s,J,envelope,tuple(np.array(envelope_params)+delta_x*np.array([1,0]))))\n",
    "r_4 = composite_likelihoods(joints,J,mu_L,V_L,Sigma_tilde_func(s,J,envelope,tuple(np.array(envelope_params)+delta_x*np.array([0,1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c7b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3699faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all = np.array([r,(r_1-r)/delta_x, (r_2 - r)/ delta_x, (r_3 - r)/ delta_x, (r_4 - r)/delta_x])\n",
    "np.save(arr=r_all,file ='r_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wanted-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_trawl_function(envelope,envelope_params):\n",
    "    assert envelope in ['exponential','gamma','ig']\n",
    "    assert isinstance(envelope_params,tuple) and isinstance(jump_part_params,tuple)\n",
    "\n",
    "    if envelope == 'exponential':\n",
    "    \n",
    "        assert len(envelope_params) == 1\n",
    "        lambda_  = envelope_params[0]\n",
    "        trawl_function = lambda x :  lambda_ * np.exp(x * lambda_) * (x<=0)\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        \n",
    "        assert len(envelope_params) == 2\n",
    "        H,delta = envelope_params\n",
    "        trawl_function = lambda x :  H/delta * (1-x/delta)**(-H-1) * (x<=0)\n",
    "        \n",
    "    elif envelope == 'ig':\n",
    "        \n",
    "        assert len(envelope_params) == 2\n",
    "        gamma,delta = envelope_params\n",
    "        #total_area = gamma/delta change of varialbe ()**-0.5 = z\n",
    "        trawl_function = lambda x : (delta/gamma) * (1-2*x/gamma**2)**(-0.5) *np.exp(delta * gamma * (1- (1-2*x/gamma**2)**0.5)) * (x<=0)\n",
    "        \n",
    "    return trawl_function\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "nr_simulations   = 2\n",
    "nr_trawls = 200\n",
    "tau = 0.35\n",
    "decorrelation_time = -np.inf\n",
    "gaussian_part_params = (1.75,2.5)\n",
    "jump_part_name   = None \n",
    "jump_part_params = (0,0)\n",
    "#envelope parameters\n",
    "envelope =  'exponential'    #envelope  is one of  ['exponential','gamma','ig']\n",
    "TRUE_ENVELOPE_PARAMS  = (1.,)\n",
    "trawl_function = define_trawl_function(envelope,TRUE_ENVELOPE_PARAMS)\n",
    "J = np.array((0,1))\n",
    "#np.random.seed(0)\n",
    "trawl_slice = trawl(nr_trawls = nr_trawls, nr_simulations = nr_simulations,\n",
    "                    trawl_function = trawl_function, tau =  tau,\n",
    "               decorrelation_time =  decorrelation_time, gaussian_part_params = gaussian_part_params,\n",
    "               jump_part_name = jump_part_name ,jump_part_params = jump_part_params)   \n",
    "\n",
    "trawl_slice.simulate('slice','diagonals')\n",
    "simulation_nr_to_use=0\n",
    "values = trawl_slice.values[simulation_nr_to_use] \n",
    "joints = [values[i+J] for i in range(nr_trawls - max(J))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5cfbbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.309789267861551 7.425597879461895\n",
      "2.3097892678615506 5.009235286358456\n",
      "2.309789267861551 5.724976421201918\n",
      "2.3097892678615515 5.755317231597498\n",
      "2.309789267861551 5.796083674982654\n",
      "2.309789267861551 5.793904471295346\n",
      "2.309789267861551 5.7939658779112335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 835.2947759496554\n",
       "        x: [ 8.588e-01]\n",
       "      nit: 5\n",
       "      jac: [ 2.530e-06]\n",
       "     nfev: 7\n",
       "     njev: 7\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimise_l_hat(tau,J,envelope,(0.5,),joints,nr_trawls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "rolled-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.309789267861552 5.894744840820119\n",
      "2.3097892678615515 5.894772258121317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.726907769101672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000* (l_hat(tau,J,envelope,(0.82,),joints,nr_trawls) - l_hat(tau,J,envelope,(0.81999,),joints,nr_trawls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3bfc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6146578999566543"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_l_hat(tau,J,envelope,(0.8214884,),joints,nr_trawls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84d2f5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3097892678615515, 4.848205112204911)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_L_V_L(tau,J,envelope,(2.5,),joints,nr_trawls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0fa3c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lags_to_use' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlags_to_use\u001b[49m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'lags_to_use' is not defined"
     ]
    }
   ],
   "source": [
    "lags_to_use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3201e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with zeros in u_CL\n",
    "indeces = [data_frames_list[i].u_CL == 0 for i in range(len(data_frames_list))]\n",
    "index_1 = indeces[0].copy()\n",
    "for i in range(1,len(indeces)):\n",
    "    index_1 = np.logical_or(index_1,indeces[i])\n",
    "\n",
    "print(f'we exclude {sum(index_1)}/{nr_simulations} simulations')\n",
    "\n",
    "processed_data_frames_list = [i[~index_1] for i in data_frames_list]\n",
    "true_df = pd.DataFrame(data = [(envelope_params_true + gaussian_part_params_true)*2],columns=['u_GMM','mu_GMM','scale_GMM','u_CL','mu_CL','scale_CL'])\n",
    "true_df_extended = pd.DataFrame(data = [(envelope_params_true + gaussian_part_params_true)*2],columns=['u_GMM','mu_GMM','scale_GMM','u_CL','mu_CL','scale_CL'],index = k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b964399",
   "metadata": {},
   "source": [
    "######  TO DO: ACF PLOTS FOR BAD CASES ###############\n",
    "###### CHECK u = 0.1 #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#median_error = pd.concat([np.median(processed_data_frames_list[i] - true_df_extended,axis=0)  for i in range(len(processed_data_frames_list))])\n",
    "#edian_error.index = list(range(len(processed_data_frames_list)))\n",
    "data_frames_list[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be48475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concatenate([np.median(processed_data_frames_list[i]- true_df_extended,axis=0)  for i in range(len(processed_data_frames_list))])#,\\\n",
    "             #columns=['u_GMM','mu_GMM','scale_GMM','u_CL','mu_CL','scale_CL'])#,index=k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([np.mean(np.square((processed_data_frames_list[i]- true_df.values)),axis=0)  for i in range(len(processed_data_frames_list))],\\\n",
    "             columns =['u_GMM','mu_GMM','scale_GMM','u_CL','mu_CL','scale_CL'],index = k_list)\n",
    "\n",
    "#GMM problems with higher lags because we didn t divide by the abs value of the correlation probably (not confident)\n",
    "#mu_CL might increase after 10 lags due to problems with approximating \\Sigma inverse with small nr of trawls on a delta grid\n",
    "#scale_CL keeps decreasing even for lags >20 and shows convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad0b15",
   "metadata": {},
   "source": [
    "### do histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b039a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### print(np.median(np.square(df_exp.u_GMM -u)))\n",
    "#print(np.median(np.square(df_exp.u_CL -u)))\n",
    "#data_frames_list[12][120:150]\n",
    "import pickle\n",
    "\n",
    "with open('data_frames_list_pickled.pkl', 'wb') as f:\n",
    "      pickle.dump(data_frames_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.median(np.abs(df_exp.mu_GMM -gaussian_part_params1[0])))\n",
    "#print(np.median(np.abs(df_exp.mu_CL -gaussian_part_params1[0])))\n",
    "with open('data_frames_list_pickled.pkl', 'rb') as f:\n",
    "       mynewlist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.median(np.abs(df_exp.scale_GMM -gaussian_part_params1[1])))\n",
    "#print(np.median(np.abs(df_exp.scale_CL -gaussian_part_params1[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_exp.isnull().any().any()\n",
    "[pd.DataFrame.equals(data_frames_list[i],mynewlist[i]) for i in range(19)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-buying",
   "metadata": {},
   "source": [
    "#### The loss function looks monotonic for exponential trawls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dsk\n",
    "#\n",
    "values_exp = trawl_exponential_1.result[-3]\n",
    "k_exp=2\n",
    "joints_exp = [np.array(values_exp[i:i+k_exp]) for i in range(0,len(values_exp) - k_exp +1 )]\n",
    "\n",
    "x = np.linspace(max(u-2,0.1),u+2,100)\n",
    "z = [l_hat(s,k_exp,'exponential',[i],joints_exp) for i in x]\n",
    "plt.plot(x,z,label='loss')\n",
    "plt.axvline(x=u,color = 'r',label='u')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-times",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_gamma = trawl_gamma_1.result[-3]\n",
    "k_gamma=10\n",
    "joints_gamma = [np.array(values[i:i+k_gamma]) for i in range(0,len(values_gamma) - k_gamma +1 )]\n",
    "\n",
    "x=np.linspace(max(0.1,H-0.4),H+0.4,75)\n",
    "y = np.linspace(max(0.1,delta-0.6), delta + 0.6, 75)\n",
    "X,Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.array([[l_hat(s,k_gamma,'gamma',[X[i][j],Y[i][j]],joints_gamma) for i in range(len(X))] for j in range(len(Y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-sellers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(X, Y, Z, cmap='RdGy')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hat(0.1,10,'exponential',(1.25,),((1,2),(2,3),(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-miami",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.log(np.linalg.det(Sigma_tilde_func(0.1,30,'exponential',(1.25,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log((1-np.exp(-2 * 1.25 * 0.1))**(30-1))\n",
    "k=10\n",
    "values = trawl.result[-3]\n",
    "joints = [np.array(values[i:i+k]) for i in range(0,len(values) - k +1 )]\n",
    "envelope = 'exponential'\n",
    "envelope_params = (1.25,)\n",
    "s=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hat(s,k,envelope,envelope_params,joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_l_hat(0.5,10,envelope,envelope_params,joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be61000",
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29039a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(Sigma_tilde_func(tau,np.array([0,1,2,3]),envelope,(2.25,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-20*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
