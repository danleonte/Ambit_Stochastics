{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eacc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jax_aux_file_jitted_25_01_2023 = import_file(os.path.join(Path(os.getcwd()).parent,'jax_aux_file_jitted_25_01_2023.py'))\n",
    "def corr_np(s,envelope,envelope_params):\n",
    "    \"\"\"return overlap area (i.e. correlation) Corr(X_t,X_{t+s}). these are equivalent because the area of the \n",
    "    ambit set is normalised to 1. s >0\"\"\"\n",
    "\n",
    "    assert envelope in ['gamma','exponential','ig']\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        u = envelope_params[0]\n",
    "        area = np.exp(- u * s)\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        H,delta = envelope_params\n",
    "        area = (1+s/delta)**(-H)\n",
    "        \n",
    "    elif envelope == 'ig':\n",
    "        gamma,delta =  envelope_params\n",
    "        area = np.exp(delta * gamma *(1-np.sqrt(2*s/gamma**2+1)))\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e6d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 14:07:06.800350: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default precision is:  <class 'jax.numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "#generic imports\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#ambit stochastics imports\n",
    "from generate_trawls_with_gaussian_marginal import generate_gaussian_seed_trawls\n",
    "#gmm fitting of envelope params + gmm / mle for levy seed params\n",
    "from ambit_stochastics.helpers.marginal_distribution_functions import fit_trawl_marginal\n",
    "from ambit_stochastics.helpers.acf_functions import fit_trawl_envelope_gmm\n",
    "import pickle\n",
    "\n",
    "#jax imports\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from bfgs_for_cl_helper import do_modified_bfgs\n",
    "from   jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "default_precision = jnp.float64\n",
    "print('Default precision is: ',default_precision)\n",
    "\n",
    "\n",
    "                         \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ##########################simulation study parameters#########################\n",
    "    \n",
    "    #trawl simulation parameters\n",
    "    tau = 1\n",
    "    nr_trawls = 2000\n",
    "    nr_simulations = 2\n",
    "    TRUE_GAUSSIAN_PARAMS = (4, 3)\n",
    "    assert TRUE_GAUSSIAN_PARAMS[1] > 0\n",
    "    envelope = 'gamma'\n",
    "    jax_seed = 4564340345993\n",
    "    key = random.PRNGKey(jax_seed)\n",
    "\n",
    "    #trawl function parameters\n",
    "    TRUE_ENVELOPE_PARAMS = (0.5,0.75)\n",
    "    np.random.seed(seed = 36363)\n",
    "    \n",
    "    np_random_seeds = np.random.randint(low = 1, high = 2**31, size = 1)\n",
    "\n",
    "        \n",
    "    #inference params\n",
    "    nr_mc_samples_per_batch =    10**3\n",
    "    nr_batches =  10 #giving nr_mc_samples_per_batch * nr_batches total samples \n",
    "    max_taylor_deg = 3  # degree of the taylor polynomial used as control variate\n",
    "    \n",
    "    #bfgs params\n",
    "    max_iter_at_once_bfgs = 20\n",
    "    max_batches_bfgs      = 1\n",
    "    \n",
    "\n",
    "    lags_list = ((1,3,5,10,15),)#(1,3,5,10),(1,3,5))#,(1,5,10),(1,5,10,15),(1,5,10,15,20)),(1,3,5,10,15),(1,3,5,10,20))\n",
    "    n_values = (1500,) #(1500,1000,500,250)#(1000,500,250,150)#,1000, 2500, 5000)#,750,1000,1500)\n",
    "    assert max(n_values) <= nr_trawls\n",
    "    \n",
    "    #results containers\n",
    "    levy_seed_params_list = []\n",
    "    d_gmm = dict()\n",
    "    d_cl  = dict()\n",
    "    \n",
    "    \n",
    "    #simulate the trawl process\n",
    "    #change the np_random_seed if doing a simulation study with more \n",
    "    trawl_instance =  generate_gaussian_seed_trawls(tau = tau,nr_simulations = nr_simulations,\n",
    "                      nr_trawls = nr_trawls, envelope = envelope,envelope_params = TRUE_ENVELOPE_PARAMS,\n",
    "                      gaussian_part_params = TRUE_GAUSSIAN_PARAMS,np_seed = np_random_seeds[0])  \n",
    "    #need to change np_random_seeds[-1] and the key in jax if doing a simulation study \n",
    "  \n",
    "    all_values_not_to_use_in_general = trawl_instance.values\n",
    "\n",
    "    with open('values_par2.npy', 'wb') as fff:\n",
    "    \tnp.save(fff, all_values_not_to_use_in_general)\n",
    "    \n",
    "\n",
    "    #fit the gmm model and time it\n",
    "    start_gmm = time.time()\n",
    "\n",
    "    #marginal distribution gmm firstly\n",
    "    for n_index in range(len(n_values)):\n",
    "        n_to_use      = n_values[n_index]\n",
    "        values_to_use = all_values_not_to_use_in_general[:,:n_to_use]\n",
    "        levy_seed_params = fit_trawl_marginal(simulations = values_to_use, levy_seed = 'gaussian', method='MM')\n",
    "        levy_seed_params_list.append(levy_seed_params)\n",
    "        \n",
    "    #envelope gmm secondly\n",
    "    for lags_index in range(len(lags_list)):\n",
    "        with open(\"text.txt\",\"a\") as file:\n",
    "            file.write('lags are' +str(lags_list[lags_index]) + '\\n')\n",
    "        for n_index in range(len(n_values)):\n",
    "            \n",
    "\n",
    "            lags_to_use   = lags_list[lags_index] \n",
    "            n_to_use      = n_values[n_index]\n",
    "            values_to_use = all_values_not_to_use_in_general[:,:n_to_use]\n",
    "            \n",
    "            if lags_index ==0 and n_index == 0:\n",
    "                initial_guess = None\n",
    "                \n",
    "            elif lags_index > 0:\n",
    "                previous_lags = lags_list[lags_index-1]\n",
    "                initial_guess = None #tuple([tuple(i) for i in d_gmm[(previous_lags, n_to_use)]['envelope_params']])\n",
    "\n",
    "                \n",
    "            elif lags_index == 0 and n_index > 0:\n",
    "                previous_n    = n_values[n_index-1]\n",
    "                initial_guess = None #tuple([tuple(i) for i in d_gmm[(lags_to_use, previous_n)]['envelope_params']])               \n",
    "\n",
    "            else:\n",
    "                raise ValueError('we go home')\n",
    "                \n",
    "\n",
    "\n",
    "            envelope_params  = fit_trawl_envelope_gmm(s = tau,simulations = values_to_use, lags = lags_to_use,\n",
    "                                                      envelope = envelope)#, initial_guess = initial_guess)\n",
    "                                               \n",
    "\n",
    "            d_gmm[(lags_to_use,n_to_use)] = {'envelope_params':envelope_params,'levy_seed_params': levy_seed_params_list[n_index]}\n",
    "            \n",
    "    end_gmm = time.time()\n",
    "    with open(\"text.txt\",\"a\") as file:\n",
    "        file.write('gmm fitting finished, time taken: ' + str((end_gmm - start_gmm)//60) + ' minutes \\n')      \n",
    "    \n",
    "    \t        \n",
    "    #fit the cl model\n",
    "    for lags_index in range(len(lags_list)):\n",
    "        start_current_lag = time.time()        \n",
    "        \n",
    "        for n_index in range(len(n_values)):\n",
    "            with open(\"text.txt\",\"a\") as file:\n",
    "                file.write('lags_index is: ' + str(lags_index) +'\\n')\n",
    "                file.write('n_index is: ' + str(n_index))\n",
    "\n",
    "\n",
    "            #keep track of parameters and loss: not at the moment\n",
    "            results_list     = []\n",
    "            #loss_bfgs       = []\n",
    "            #parameters_bfgs = []\n",
    "            #hessian_list    = []\n",
    "            \n",
    "            for simulation_to_use in range(nr_simulations): \n",
    "                with open(\"text.txt\",\"a\") as file:\n",
    "\n",
    "                    file.write('simulation ' + str(simulation_to_use) + ' / ' + str(nr_simulations) +'\\n')\n",
    "            \n",
    "                 \n",
    "\n",
    "                lags_to_use   = lags_list[lags_index] \n",
    "                n_to_use      = n_values[n_index]\n",
    "                values_to_use = all_values_not_to_use_in_general[simulation_to_use,:n_to_use]  \n",
    "                \n",
    "                #initialize model parameters with gmm result\n",
    "                _ = d_gmm[(lags_to_use,n_to_use)] \n",
    "                #print(_['levy_seed_params'][simulation_to_use])\n",
    "                \n",
    "                #initial_tensor = np.concatenate([[_['levy_seed_params'][simulation_to_use][0],\n",
    "                #                                 1/_['levy_seed_params'][simulation_to_use][1]],\n",
    "                 #                               _['envelope_params'][simulation_to_use]])\n",
    "                    \n",
    "                #initial_log_tensor = jnp.log(initial_tensor.copy())\n",
    "                \n",
    "                initial_tensor = np.concatenate([[_['levy_seed_params'][simulation_to_use][0],\n",
    "                                                 np.log(_['levy_seed_params'][simulation_to_use][1])],\n",
    "                                                np.log(_['envelope_params'][simulation_to_use])])       \n",
    "                \n",
    "                initial_transformed_tensor = jnp.array(initial_tensor.copy())\n",
    "\n",
    "                \n",
    "\n",
    "                try:\n",
    "                    resdd, key = do_modified_bfgs(trawl_path = values_to_use, envelope = envelope,\n",
    "                                    tau = tau, nr_mc_samples_per_batch = nr_mc_samples_per_batch, nr_batches = nr_batches,\n",
    "                                    max_taylor_deg = max_taylor_deg, key = key, lags_list = lags_to_use,x0 = initial_transformed_tensor, \n",
    "                                    max_iter_at_once_bfgs = max_iter_at_once_bfgs, max_batches_bfgs = max_batches_bfgs)\n",
    "                    \n",
    "                    results_list.append(resdd)\n",
    "\n",
    "                except ValueError:\n",
    "                    for splitting_index in range(100):\n",
    "                        key, subkey = jax.random.split(key)\n",
    "                    try:\n",
    "                        resdd, key = do_modified_bfgs(trawl_path = values_to_use, envelope = envelope,\n",
    "                                    tau = tau, nr_mc_samples_per_batch = nr_mc_samples_per_batch, nr_batches = nr_batches,\n",
    "                                    max_taylor_deg = max_taylor_deg, key = key, lags_list = lags_to_use,x0 = initial_transformed_tensor, \n",
    "                                    max_iter_at_once_bfgs = max_iter_at_once_bfgs, max_batches_bfgs = max_batches_bfgs)\n",
    "                        \n",
    "                        results_list.append(resdd)\n",
    "\n",
    "                    except ValueError:\n",
    "                        with open(\"text.txt\",\"a\") as file:\n",
    "                            file.write('simulation ' + str(simulation_to_use) +  ' is very problematic')\n",
    "                        results_list.append(np.nan)\n",
    "\n",
    "                        #loss_bfgs_to_add,parameters_bfgs_to_add = np.nan,initial_tensor.copy()\n",
    "\n",
    "\n",
    "                #loss_bfgs.append(loss_bfgs_to_add)\n",
    "                #parameters_bfgs.append(parameters_bfgs_to_add)\n",
    "\n",
    "\n",
    "            #d_cl[(lags_to_use,n_to_use)] = {'loss':loss_bfgs,'params': parameters_bfgs}\n",
    "            d_cl[(lags_to_use,n_to_use)] = results_list\n",
    "\n",
    "        \n",
    "        end_current_lag = time.time()      \n",
    "        \n",
    "        with open(\"text.txt\",\"a\") as file:\n",
    "            file.write('current lags time was: ' + str((end_current_lag - start_current_lag)//60) + '\\n')\n",
    "        \n",
    "    end_cl = time.time()\n",
    "    with open(\"text.txt\",\"a\") as file:\n",
    "        file.write('cl fitting finished, time taken: ' +  str((end_cl - end_gmm)//60) + ' minutes \\n')     \n",
    "    with open(\"cl_dictionary.pickle\", \"wb\") as output_file_cl:\n",
    "        pickle.dump(d_cl, output_file_cl)\n",
    "    with open(\"gmm_dictionary.pickle\",\"wb\") as output_file_gmm:\n",
    "        pickle.dump(d_gmm, output_file_gmm)\n",
    "\n",
    "    #write cl_time to disk\n",
    "    cl_time = [end_current_lag - end_gmm]\n",
    "    with open(\"cl_time.pickle\", \"wb\") as output_cl_time:\n",
    "        pickle.dump(cl_time, output_cl_time)\n",
    "\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab81d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5787cf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 16:49:16.889726: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-21 16:49:16.930203: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-21 16:49:16.932194: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default precision is in jax_aux file is:  <class 'jax.numpy.float64'>\n",
      "Default precision is:  <class 'jax.numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "#generic imports\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#ambit stochastics imports\n",
    "from generate_trawls_with_gaussian_marginal import generate_gaussian_seed_trawls\n",
    "#gmm fitting of envelope params + gmm / mle for levy seed params\n",
    "from ambit_stochastics.helpers.marginal_distribution_functions import fit_trawl_marginal\n",
    "from ambit_stochastics.helpers.acf_functions import fit_trawl_envelope_gmm\n",
    "import pickle\n",
    "\n",
    "#jax imports\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from bfgs_for_cl_helper import do_modified_bfgs\n",
    "from   jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "default_precision = jnp.float64\n",
    "print('Default precision is: ',default_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3752ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TRUE_GAUSSIAN_PARAMS = (3, 5)\n",
    "TRUE_ENVELOPE_PARAMS = (0.1,)\n",
    "envelope = 'exponential'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "98da1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_to_use = 4\n",
    "with open(os.path.join('lambda_0_1','gmm_dictionary.pickle'), 'rb') as f:\n",
    "    x_gmm = pickle.load(f)\n",
    "env_infer_gmm = list(x_gmm.values())[sim_to_use]['envelope_params']\n",
    "levy_seed_infer_gmm = list(x_gmm.values())[sim_to_use]['levy_seed_params']\n",
    "#env_infer_gmm, levy_seed_infer_gmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "805f4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('lambda_0_1','cl_dictionary.pickle'), 'rb') as f:\n",
    "    x_cl = pickle.load(f)\n",
    "x_cl = list(x_cl.values())[sim_to_use]\n",
    "levy_seed_infer_cl = np.array([[i.x[0],np.exp(i.x[1])] for i in x_cl])\n",
    "env_infer_cl = np.array([np.exp(i.x[2:]) for i in x_cl])\n",
    "#env_infer_cl,\n",
    "#levy_seed_infer_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "71deb085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=100, minmax=(array([-22.51754825]), array([33.83887847])), mean=array([-3.23419469]), variance=array([83.93208537]), skewness=array([0.75339526]), kurtosis=array([1.94692807]))"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(os.path.join('lambda_0_1','cl_dictionary.pickle'), 'rb') as f:\n",
    "    x_cl = pickle.load(f)\n",
    "#x_cl\n",
    "from scipy import stats\n",
    "\n",
    "stats.describe(100 *(env_infer_cl -env_infer_gmm) / env_infer_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "aaf04623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "def evaluate_acf_loss(envelope,envelope_params,TRUE_ENVELOPE_PARAMS,k):\n",
    "  envelope_combined_loss = []\n",
    "  for i in range(len(envelope_params)):\n",
    "        func_sq_er  = lambda s : (corr_np(s,envelope,envelope_params[i]) - corr_np(s,envelope,TRUE_ENVELOPE_PARAMS))**2 / (k*s**2+1)\n",
    "        func_abs_er = lambda s : abs(corr_np(s,envelope,envelope_params[i]) - corr_np(s,envelope,TRUE_ENVELOPE_PARAMS)) /(k*s**2+1)\n",
    "        \n",
    "        int_sq_er = quad(func_sq_er,0,np.inf,limit = 1000,maxp1=1000, limlst=1000,epsabs=1.49e-04, epsrel=1.49e-04)[0]\n",
    "        int_abs_er= quad(func_abs_er,0,np.inf,limit = 1000,maxp1=1000, limlst=1000,epsabs=1.49e-04, epsrel=1.49e-04)[0]\n",
    "        envelope_combined_loss.append([int_sq_er**0.5,int_abs_er])\n",
    "  return envelope_combined_loss\n",
    "\n",
    "r_gmm = evaluate_acf_loss(envelope,env_infer_gmm,TRUE_ENVELOPE_PARAMS,0.00001)\n",
    "r_cl  = evaluate_acf_loss(envelope,env_infer_cl,TRUE_ENVELOPE_PARAMS,0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "85c5ceb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.95052541, 0.96389776]), array([0.9580828 , 0.95891937]))"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r_cl,axis=0) /np.mean(r_gmm,axis=0),np.median(r_cl,axis=0) /np.median(r_gmm,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1c725bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('lambda_0_1','cl_dictionary.pickle'), 'rb') as f:\n",
    "    x_cl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7e6ae1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67874262])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(env_infer_cl - TRUE_ENVELOPE_PARAMS)**2,axis=0)/np.mean(np.abs(env_infer_gmm - TRUE_ENVELOPE_PARAMS)**2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "d3f4c1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9010087]), array([0.19194658]))"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(env_infer_cl - TRUE_ENVELOPE_PARAMS),axis=0)/np.median(np.abs(env_infer_gmm - TRUE_ENVELOPE_PARAMS),axis=0),np.median(np.abs(env_infer_gmm - TRUE_ENVELOPE_PARAMS)) /TRUE_ENVELOPE_PARAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "14684b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89181823])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(env_infer_cl - TRUE_ENVELOPE_PARAMS),axis=0)/np.mean(np.abs(env_infer_gmm - TRUE_ENVELOPE_PARAMS),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "57b03fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98895992, 0.99687871])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(levy_seed_infer_cl - TRUE_GAUSSIAN_PARAMS)**2,axis=0)/np.mean(np.abs(levy_seed_infer_gmm - TRUE_GAUSSIAN_PARAMS)**2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "1fa08e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9983537 , 0.99680582])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(levy_seed_infer_cl - TRUE_GAUSSIAN_PARAMS),axis=0)/np.mean(np.abs(levy_seed_infer_gmm - TRUE_GAUSSIAN_PARAMS),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "95f2eda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96443978, 1.03613012])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(levy_seed_infer_cl - TRUE_GAUSSIAN_PARAMS),axis=0)/np.median(np.abs(levy_seed_infer_gmm - TRUE_GAUSSIAN_PARAMS),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "2fa56942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(mu_1,sigma_1,mu_2,sigma_2):\n",
    "    return np.log(sigma_2/sigma_1)+ (sigma_1**2+(mu_1-mu_2)**2)/(2*sigma_2**2)-1/2\n",
    "\n",
    "r_cl = kl_div(TRUE_GAUSSIAN_PARAMS[0],TRUE_GAUSSIAN_PARAMS[1],levy_seed_infer_cl[:,0],levy_seed_infer_cl[:,1])\n",
    "r_gmm = kl_div(TRUE_GAUSSIAN_PARAMS[0],TRUE_GAUSSIAN_PARAMS[1],levy_seed_infer_gmm[:,0],levy_seed_infer_gmm[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "b9288df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.995356716905765, 1.0089398737396726)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cl.sum()/ r_gmm.sum(), np.median(r_cl) / np.median(r_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "606f5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#((r_cl**2).sum()/(r_gmm**2).sum())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2620a380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "527be7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.123375530167354"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ec1c9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1362208576196877"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd083389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
