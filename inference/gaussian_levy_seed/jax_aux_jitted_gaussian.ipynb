{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4814a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 10:44:19.996913: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-19 10:44:20.307149: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-19 10:44:20.315213: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default precision is in jax_aux file is:  <class 'jax.numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from jax import jacfwd #grad, jacrev, value_and_grad\n",
    "#from jax import jit, random, vmap, pmap\n",
    "from functools import partial\n",
    "from jax.lax import stop_gradient\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "#--------------------------------------------------\n",
    "default_precision = jnp.float64\n",
    "print('Default precision is in jax_aux file is: ',default_precision)\n",
    "#--------------------------------------------------\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "#distributions\n",
    "Normal  = tfp.distributions.Normal\n",
    "\n",
    "def create_joints_jax(path,lag):\n",
    "    assert isinstance(lag, int),'variable <lag> is not integer'\n",
    "    path_length = len(path)\n",
    "    pairs =  jnp.array([sorted([path[i],path[i+lag]]) for i in range(path_length-lag)])\n",
    "    return pairs\n",
    "\n",
    "def create_joints_jax_with_padding(path,lag):\n",
    "    #print('LAG IS ')\n",
    "    #print(lag)\n",
    "    assert isinstance(lag, int),'variable <lag> is not integer'\n",
    "    assert lag > 0,'variable <lag> should be positive'\n",
    "    assert lag < len(path)\n",
    "    path_length = len(path)\n",
    "    pairs_unpadded = [sorted([path[i],path[i+lag]]) for i in range(path_length-lag)] \n",
    "    if lag > 1:\n",
    "      padding = [pairs_unpadded[-1].copy() for j in range(lag-1)]\n",
    "      return jnp.array(pairs_unpadded + padding)\n",
    "\n",
    "    if lag == 1:\n",
    "      return jnp.array(pairs_unpadded)\n",
    "\n",
    "@partial(jax.jit, static_argnames=['envelope'])\n",
    "def corr_jax(s,envelope,envelope_params):\n",
    "    \"\"\"return overlap area (i.e. correlation) Corr(X_t,X_{t+s}). these are equivalent because the area of the \n",
    "    ambit set is normalised to 1. s >0\"\"\"\n",
    "\n",
    "    assert envelope in ['gamma','exponential','ig']\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        u = envelope_params[0]\n",
    "        area = jnp.exp(- u * s)\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        H,delta = envelope_params\n",
    "        area = (1+s/delta)**(-H)\n",
    "        \n",
    "    elif envelope == 'ig':\n",
    "        gamma,delta =  envelope_params\n",
    "        area = jnp.exp(delta * gamma *(1-jnp.sqrt(2*s/gamma**2+1)))\n",
    "    return area\n",
    "\n",
    "\n",
    "def corr_np(s,envelope,envelope_params):\n",
    "    \"\"\"return overlap area (i.e. correlation) Corr(X_t,X_{t+s}). these are equivalent because the area of the \n",
    "    ambit set is normalised to 1. s >0\"\"\"\n",
    "\n",
    "    assert envelope in ['gamma','exponential','ig']\n",
    "    \n",
    "    if envelope == 'exponential':\n",
    "        u = envelope_params[0]\n",
    "        area = np.exp(- u * s)\n",
    "        \n",
    "    elif envelope == 'gamma':\n",
    "        H,delta = envelope_params\n",
    "        area = (1+s/delta)**(-H)\n",
    "        \n",
    "    elif envelope == 'ig':\n",
    "        gamma,delta =  envelope_params\n",
    "        area = np.exp(delta * gamma *(1-np.sqrt(2*s/gamma**2+1)))\n",
    "    return area\n",
    "\n",
    "\n",
    "#______________________ likelihood approximation _________________#\n",
    "@partial(jax.jit, static_argnames=['envelope'])\n",
    "def get_sampling_params(transformed_params_1,tau_,envelope):\n",
    "    transformed_levy_params = transformed_params_1[:2]\n",
    "    log_env_params          = transformed_params_1[2:]\n",
    "        \n",
    "    rho = corr_jax(tau_, envelope, jnp.exp(log_env_params))\n",
    "        \n",
    "    mu, sigma = transformed_levy_params[0], jnp.exp(transformed_levy_params[1])\n",
    "    \n",
    "    mu_0    = mu    * rho\n",
    "    sigma_0 = sigma * jnp.sqrt(rho)\n",
    "\n",
    "    return mu_0,sigma_0\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['envelope','nr_samples'])\n",
    "def sample_z(transformed_params_1_, tau_, l1, envelope, nr_samples, key_):\n",
    "    mu_0, sigma_0 = get_sampling_params(transformed_params_1_,tau_,envelope)\n",
    "\n",
    "    sampler = Normal(loc = mu_0, scale = sigma_0)\n",
    "    z  = sampler.sample([len(l1), nr_samples] , seed = key_)  #don't forget to use subkey to generate new samples\n",
    "\n",
    "    return z \n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['envelope'])\n",
    "def f(z, tau_, envelope, transformed_params_, l1, l2):\n",
    "        \n",
    "    transformed_levy_params = transformed_params_[:2]\n",
    "    log_env_params          = transformed_params_[2:]\n",
    "        \n",
    "    rho = corr_jax(tau_, envelope, jnp.exp(log_env_params))    \n",
    "    mu, sigma = transformed_levy_params[0], jnp.exp(transformed_levy_params[1])\n",
    "    \n",
    "    mu_0, mu_1        = mu    * rho, mu * (1-rho)\n",
    "    sigma_0, sigma_1  = sigma * jnp.sqrt(rho), sigma * jnp.sqrt(1 - rho)\n",
    "          \n",
    "    multiplying_constant  = 1 / (2 * jnp.pi * sigma_1**2)\n",
    "\n",
    "    intermediary = ((jnp.expand_dims(l1,axis=1) - z) - mu_1)**2 + ((jnp.expand_dims(l2,axis=1) - z) - mu_1)**2\n",
    "    inside_exp = jnp.exp(- intermediary / (2 * sigma_1**2))   \n",
    "    #exp = jnp.expand_dims(multiplying_constant,axis=1) * inside_exp  \n",
    "    exp = multiplying_constant * inside_exp\n",
    "    \n",
    "    return exp\n",
    "\n",
    "\n",
    "#main function\n",
    "@partial(jax.jit, static_argnames=['envelope','nr_samples'])\n",
    "def estimators_likelihood_CV_demo(transformed_params_, transformed_params_1_, pairs, envelope, tau, nr_samples, key):\n",
    "\n",
    "  l1,l2 = pairs[:,0], pairs[:,1]\n",
    "\n",
    "  z           = sample_z(transformed_params_1_ = transformed_params_1_, tau_ = tau, l1 = l1, envelope = envelope, nr_samples = nr_samples, key_ = key)\n",
    "  likelihood  = f(z = z, tau_ = tau, envelope = envelope, transformed_params_ = transformed_params_, l1 = l1, l2 = l2)\n",
    "\n",
    "  return likelihood, z\n",
    "\n",
    "likelihood_jacobians = jax.jit(jacfwd(estimators_likelihood_CV_demo, argnums = (0,1)), static_argnames=['envelope','nr_samples'])\n",
    " #jax.jit(jacfwd(estimators_likelihood_CV_demo, argnums = (0,1)), static_argnames=['envelope','nr_samples','max_taylor_deg'])\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['envelope','max_taylor_deg'])  \n",
    "def compute_taylor_coeff_new(base_point,tau_, envelope, transformed_params_, max_taylor_deg, pairs):\n",
    "  \"\"\" the beta samples z are already scaled by l_1\"\"\"\n",
    "  l1,l2 = pairs[:,0], pairs[:,1]\n",
    "  f_ = f \n",
    "  g_ = jacfwd(f_, argnums = 3)\n",
    "\n",
    "\n",
    "  l = [f_(base_point, tau_, envelope,  transformed_params_, l1, l2)[:,0]]\n",
    "  #print('shape is', l[0].shape)\n",
    "\n",
    "  nabla_theta_l = [g_(base_point, tau_, envelope,  transformed_params_, l1, l2)[:,0]]\n",
    "  #print('shape is', nabla_theta_l[0].shape)\n",
    "\n",
    "\n",
    "  for i in range(1,max_taylor_deg+1):\n",
    "    f_ = jacfwd(f_, argnums = 0)  \n",
    "    g_ = jacfwd(f_, argnums = 3)\n",
    "\n",
    "    l.append(f_(base_point, tau_, envelope,  transformed_params_, l1, l2)[:,0]/ scipy.special.factorial(i)) #use some vmap\n",
    "    nabla_theta_l.append(g_(base_point, tau_, envelope,  transformed_params_, l1, l2)[:,0]/ scipy.special.factorial(i))\n",
    "\n",
    "    #print('shape is', l[i].shape)\n",
    "    #print('shape is', nabla_theta_l[i].shape)\n",
    "\n",
    "\n",
    "  return jnp.stack(l,axis=1), jnp.stack(nabla_theta_l,axis=1)\n",
    "\n",
    "\n",
    "#--------------------- moments ------------------#\n",
    "def get_differentiable_moments(sigma,max_moment):\n",
    "  \"\"\"X ~ N(0, sigma^2)\"\"\"\n",
    "  #https://math.stackexchange.com/questions/1945448/methods-for-finding-raw-moments-of-the-normal-distribution\n",
    "  #length should be max_moment+2\n",
    "  result = [1,0,sigma**2,0, 3 * sigma**4 , 0 , 15 * sigma**6 , 0, 105 * sigma**8]  \n",
    "\n",
    "  return jnp.array(result[:max_moment+2])\n",
    "\n",
    "\n",
    "#def get_differentiable_moments(mu,sigma,max_moment):\n",
    "#  \"\"\"X ~ N(mu, sigma^2)\"\"\"\n",
    "#  #https://math.stackexchange.com/questions/1945448/methods-for-finding-raw-moments-of-the-normal-distribution\n",
    "#  #length should be max_moment+2\n",
    "#  result = [1,mu, mu**2 + sigma**2, mu**3 + 3 * mu * sigma**2,  mu**4 + 6 * mu**2 *sigma**2 + 3 * sigma**4,\\\n",
    "#            mu**5 + 10 *mu**3 * sigma**2 + 15* mu *sigma**4]  \n",
    "##\n",
    "#  return jnp.array(result[:max_moment+2])\n",
    "\n",
    "#import numpy as np\n",
    "#mu = -1.5\n",
    "#sigma = 4.1\n",
    "#z = np.random.normal(loc = mu, scale = sigma, size = 5 * 10**7)\n",
    "#z2 = np.array([np.mean(z**i) for i in range(6)])\n",
    "#get_differentiable_moments(mu,sigma,4) - z2,get_differentiable_moments(mu,sigma,4)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['envelope','max_taylor_deg'])\n",
    "def differentiable_moments_func(transformed_params_1, tau_, envelope, pairs, max_taylor_deg):\n",
    "\n",
    "  ___,sigma = get_sampling_params(transformed_params_1 = transformed_params_1, tau_ = tau_, envelope = envelope)\n",
    "  moments_list = get_differentiable_moments(sigma = sigma, max_moment = max_taylor_deg)\n",
    "  return jnp.transpose(jnp.array([moments_list[i_]*jnp.ones(len(pairs[:,0])) for i_ in range(0, max_taylor_deg+1)]))\n",
    "\n",
    "differentiable_moments_gradients_func = jax.jit(jacfwd(differentiable_moments_func), static_argnames=['envelope','max_taylor_deg'])\n",
    "\n",
    "#-------------------- moments ------------------#\n",
    "\n",
    "#@partial(jax.jit, static_argnames=['envelope','max_taylor_deg'])  #TO ADD BACK\n",
    "def calculate_moments_to_add(transformed_params_1, pairs, envelope, tau_, max_taylor_deg, coeff, nabla_theta_coeff):\n",
    "\n",
    "    moments_list    = differentiable_moments_func(transformed_params_1 = transformed_params_1, tau_ = tau_, envelope = envelope, pairs = pairs, max_taylor_deg = max_taylor_deg)\n",
    "    pg_moments_list = differentiable_moments_gradients_func(transformed_params_1, tau_, envelope, pairs, max_taylor_deg)\n",
    "    #shapes are [nr_samples,max_taylor_deg+1] and [nr_samples, max_taylor_deg+1, nr_params]\n",
    "    \n",
    "    #print(moments_list.shape,'n')\n",
    "    #print(coeff[:,0].shape)\n",
    "    moments_to_add_likelihood  = coeff[:,0] * moments_list[:,0]\n",
    "    moments_to_add_nabla_theta = nabla_theta_coeff[:,0] * moments_list[:,[0]]\n",
    "    moments_to_add_pg          = coeff[:,[0]] * pg_moments_list[:,0]  #first moment should be 0\n",
    "\n",
    "\n",
    "    for i_ in range(1,max_taylor_deg+1):\n",
    "        moments_to_add_likelihood  += coeff[:,i_] * moments_list[:,i_]\n",
    "        moments_to_add_nabla_theta += nabla_theta_coeff[:,i_] * moments_list[:,[i_]]\n",
    "        moments_to_add_pg          += coeff[:,[i_]] * pg_moments_list[:,i_] \n",
    "\n",
    "    return moments_to_add_likelihood, moments_to_add_nabla_theta, moments_to_add_pg   \n",
    "\n",
    "\n",
    "#---------------cl functions for cluster ----------------\n",
    "\n",
    "#@partial(jax.jit, static_argnames=['envelope','nr_samples','max_taylor_deg'])  \n",
    "#the for loop might not be suitable for jitting, as it would unfold it?\n",
    "\n",
    "def likelihood_and_grad(transformed_theta, transformed_theta_1, pairs, envelope, tau , nr_mc_samples_per_batch, nr_batches, max_taylor_deg, key ):\n",
    "  \"\"\"transformed_theta_1 goes into z and pathwise gradients\n",
    "  his code works for lag 1. to make it work for higher lags, use tau = tau * lag\n",
    "  don't forget to change the key between simulations\"\"\"\n",
    "\n",
    "  #results initialization\n",
    "  likelihood_mean  = 0#np.zeros(pairs.shape[0])\n",
    "  nabla_theta_mean = 0#np.zeros(pairs.shape[0], len(transformed_theta))\n",
    "  pg_mean          = 0#np.zeros(pairs.shape[0], len(transformed_theta))\n",
    "\n",
    "  mu_taylor_base_point = get_sampling_params(transformed_theta,tau,envelope)[0]\n",
    "\n",
    "  for batch_number in range(nr_batches): #_ is batch number\n",
    "\n",
    "    #update the random numbers!!!\n",
    "    key, subkey = jax.random.split(key)   \n",
    "    likelihood, z = estimators_likelihood_CV_demo(transformed_params_ = transformed_theta, transformed_params_1_ = transformed_theta_1, pairs = pairs, envelope = envelope,\n",
    "                                tau = tau, nr_samples = nr_mc_samples_per_batch, key = subkey) #subkey not key here\n",
    "    #get gradients\n",
    "    jacobians_f, jacobians_z =  likelihood_jacobians(transformed_theta, transformed_theta_1, pairs, envelope, tau, nr_mc_samples_per_batch, subkey)\n",
    "    nabla_theta_f, pg_f = jacobians_f  \n",
    "    zero_variable, pg_z = jacobians_z\n",
    "\n",
    "    if batch_number == 0: #if it's the 1st batch\n",
    "\n",
    "        #coefficients of taylor approximation of deg taylor_deg, which we use first to substract the control variate and then to add back its expectation\n",
    "        #shapes are [nr_samples, max_taylor_deg+1] and [nr_samples, max_taylor_deg+1, nr_params]\n",
    "        coeff, nabla_theta_coeff   = compute_taylor_coeff_new(base_point = mu_taylor_base_point, tau_ = tau, envelope = envelope, transformed_params_ = transformed_theta,\n",
    "                                                              max_taylor_deg = max_taylor_deg, pairs = pairs)\n",
    "        \n",
    "        #moments to add back (calculation of moments and initialization with the first moment)\n",
    "        moments_to_add_likelihood, moments_to_add_nabla_theta, moments_to_add_pg = calculate_moments_to_add(transformed_params_1 = transformed_theta_1, pairs = pairs, \n",
    "                                                                                     envelope = envelope, tau_ = tau, max_taylor_deg = max_taylor_deg+1,         \n",
    "                                                                                     coeff = coeff, nabla_theta_coeff = nabla_theta_coeff)\n",
    "\n",
    "    #control variate (initialization)  \n",
    "    #shapes are [nr_samples, nr_params] and [nr_samples, max_taylor_deg + 1, nr_params] and [TO ADD]\n",
    "    T_f = jnp.repeat(jnp.expand_dims(coeff[:,0], axis = 1), repeats = nr_mc_samples_per_batch, axis = 1)\n",
    "    T_nabla_theta_f = jnp.repeat(jnp.expand_dims(nabla_theta_coeff[:,0],axis=1), repeats = nr_mc_samples_per_batch, axis = 1)\n",
    "    T_pg_f          = jnp.zeros(shape = T_nabla_theta_f.shape)\n",
    "    \n",
    "    #compute control variates\n",
    "    for i_ in range(1,max_taylor_deg+1):\n",
    "      T_f += jnp.expand_dims(coeff[:,i_], axis=1) * (z-mu_taylor_base_point)**i_\n",
    "      T_nabla_theta_f += jnp.expand_dims(nabla_theta_coeff[:,i_],axis=1) * jnp.expand_dims((z-mu_taylor_base_point)**i_,axis=2)\n",
    "      T_pg_f          += i_ * jnp.expand_dims(coeff[:,[i_]] * (z-mu_taylor_base_point)**(i_-1),axis=2) * pg_z\n",
    "\n",
    "    if batch_number == 0:      #compute opimal gamma with first batch of samples\n",
    "\n",
    "      #gamma_0 =   Cov(f,T_f) / Var(T_f)  \n",
    "      demeaned_likelihood  = likelihood - jnp.nanmean(likelihood,axis=1,keepdims=True)\n",
    "      demeaned_T_f = T_f - jnp.nanmean(T_f,axis=1,keepdims=True)\n",
    "      gamma_0 = jnp.nanmean(demeaned_likelihood * demeaned_T_f,axis=1) / jnp.nanvar(T_f,axis=1)\n",
    "      \n",
    "\n",
    "      #gamma_1 = Cov(grad f,T_nabla_theta_f) / Var(T_nabla_theta_f)\n",
    "      demeaned_nabla_theta_f   = nabla_theta_f - jnp.mean(nabla_theta_f, axis=1, keepdims=True)\n",
    "      demeaned_T_nabla_theta_f = T_nabla_theta_f - jnp.mean(T_nabla_theta_f, axis=1, keepdims=True)\n",
    "      gamma_1 = jnp.nanmean(demeaned_nabla_theta_f * demeaned_T_nabla_theta_f, axis = 1) / jnp.nanvar(T_nabla_theta_f, axis= 1)\n",
    "\n",
    "      assert max_taylor_deg > 1, 'no cv used'\n",
    "        \n",
    "      #gamma_2 = Cov(df/dz * nabla_theta_z, T_df_dz * nabla_theta_z) / Var(T_df_dz * nabla_theta_z)\n",
    "      demeaned_pg_f = pg_f - jnp.mean(pg_f, axis=1, keepdims = True) #this should be df_dz * nabla_theta_z)\n",
    "      demeaned_T_pg_f = T_pg_f - jnp.mean(T_pg_f, axis =1 , keepdims = True)\n",
    "      gamma_2 = jnp.nanmean(demeaned_pg_f * demeaned_T_pg_f, axis = 1 ) / jnp.nanvar(T_pg_f, axis = 1) #makes nan's into 0's\n",
    "      gamma_2 = jnp.nan_to_num(gamma_2, nan=0.0)\n",
    "         \n",
    "\n",
    "    #likelihood and gradient calculations-------- \n",
    "    cv_likelihood = likelihood - jnp.expand_dims(gamma_0,axis=1) * T_f + jnp.expand_dims(gamma_0 * moments_to_add_likelihood,axis=1)\n",
    "    nabla_theta_component_of_gradient = nabla_theta_f - jnp.expand_dims(gamma_1,axis=1)  * T_nabla_theta_f +\\\n",
    "                   jnp.expand_dims(gamma_1 * moments_to_add_nabla_theta, axis=1)\n",
    "\n",
    "    pg_component_of_gradient    =  pg_f   - jnp.expand_dims(gamma_2,axis=1) * T_pg_f  + jnp.expand_dims(gamma_2 * moments_to_add_pg, axis=1)\n",
    "\n",
    "    #append results without control variates\n",
    "    likelihood_mean  += cv_likelihood.mean(axis=1)\n",
    "    nabla_theta_mean += nabla_theta_component_of_gradient.mean(axis=1)\n",
    "    assert max_taylor_deg > 1, 'no cv used'\n",
    "    pg_mean += pg_component_of_gradient.mean(axis=1)\n",
    "    \n",
    "  key, subkey = jax.random.split(key)   \n",
    "  return likelihood_mean / nr_batches, (nabla_theta_mean + pg_mean)/ nr_batches, key\n",
    "\n",
    "\n",
    "\n",
    "def composite_likelihood_and_grad_at_lags(transformed_theta, transformed_theta_1, trawl_path, envelope, tau , nr_samples, nr_batches, max_taylor_deg, key, lags_list):\n",
    "  \n",
    "  result_log_likelihood          = 0\n",
    "  result_gradient_log_likelihood = 0\n",
    "\n",
    "  for index in range(len(lags_list)):\n",
    "\n",
    "    lag_to_use   = lags_list[index]\n",
    "    tau_to_use   = tau * lag_to_use\n",
    "    pairs_to_use = create_joints_jax_with_padding(trawl_path, lag_to_use)\n",
    "    \n",
    "    #make sure to pass the key, as to not use the same random numbers, and to update the tau_to_use and pairs_to_use accordingly\n",
    "    #the key should be updated inside the likelihood_and_grad function\n",
    "    likelihood, grads, key = likelihood_and_grad(transformed_theta = transformed_theta, transformed_theta_1 = transformed_theta_1, pairs = pairs_to_use, envelope = envelope,\n",
    "                                                 tau = tau_to_use , nr_mc_samples_per_batch= nr_samples, nr_batches = nr_batches, max_taylor_deg = max_taylor_deg, key = key)\n",
    "    \n",
    "    total = len(likelihood)\n",
    "    likelihood = likelihood[:total - lag_to_use+1] #to check if this takes out the duplicates\n",
    "    grads      = grads[:total - lag_to_use+1,:]\n",
    "\n",
    "    result_log_likelihood           += jnp.mean(jnp.log(likelihood)) #* total / (total - lag_to_use + 1)\n",
    "    result_gradient_log_likelihood  += jnp.mean(grads / jnp.expand_dims(likelihood, axis=1), axis=0)  #* total / (total - lag_to_use + 1)\n",
    "    #print(result_log_likelihood)\n",
    "    #print(result_gradient_log_likelihood)\n",
    "    return (result_log_likelihood, result_gradient_log_likelihood)\n",
    "\n",
    "\n",
    "def minus_lambda_function_for_cl_and_grad_at_lags(trawl_path, envelope, tau , nr_samples, nr_batches, max_taylor_deg, key, lags_list):\n",
    "  \"\"\"x is a 2d array, x[0] is leg_theta and x[1] is transformed_theta_1\"\"\"\n",
    "  #print('used')\n",
    "  def lambda_(x):\n",
    "    #print(jnp.exp(x)) #doesn t actually print the value of the array, just its shape as a dynamic array\n",
    "    tuple_ =  composite_likelihood_and_grad_at_lags(x, x.copy(), trawl_path, envelope, tau , nr_samples, nr_batches, max_taylor_deg, key, lags_list)\n",
    "    return (-tuple_[0], -tuple_[1])\n",
    "\n",
    "  return lambda_\n",
    "\n",
    "\n",
    "def numpy_minus_lambda_function_for_cl_and_grad_at_lags(trawl_path, envelope, tau , nr_samples, nr_batches, max_taylor_deg, key, lags_list):\n",
    "  \"\"\"x is a 2d array, x[0] is leg_theta and x[1] is transformed_theta_1\"\"\"\n",
    "  #print('used')\n",
    "  def lambda_(x):\n",
    "    #print(x)\n",
    "    tuple_ =  composite_likelihood_and_grad_at_lags(x, x.copy(), trawl_path, envelope, tau , nr_samples, nr_batches, max_taylor_deg, key, lags_list)\n",
    "    return (-np.array(tuple_[0]), -np.array(tuple_[1]))\n",
    "\n",
    "  return lambda_  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972de702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da6708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57f9afd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118d000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46546e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74282f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d811282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ba9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
